{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58bcc8fe",
   "metadata": {},
   "source": [
    "# Kaggle NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741bfb7f",
   "metadata": {},
   "source": [
    "* Мысль такая - я приблизительно разобью текущий ноут на этапы\n",
    "* Напишу, что есть в каждом и что хотелось бы сделать\n",
    "* Текущая реализация, RMSE по 0.54, золото 0.43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a2069f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a60ac417",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning import LightningModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2830bc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d83d6497",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "719f24fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-35c4bb1e92270350\n",
      "Found cached dataset csv (/Users/belyakov/.cache/huggingface/datasets/csv/default-35c4bb1e92270350/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f0587166e3d4497bbb78a38108b0eb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_hf = load_dataset(\"csv\", data_files=\"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b553d61",
   "metadata": {},
   "source": [
    "## 1. Модель \n",
    "\n",
    "#### Хочу либо gpt-3-large либо deberta-v2-large, но мощности моего компа и коллаба не позволяют - надо думать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed54cdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/Users/belyakov/miniforge3/envs/env_tf/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68404a7d3a4842e7ad174c92f92a75d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/371M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"microsoft/deberta-v3-base\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d87a34",
   "metadata": {},
   "source": [
    "## Препроцессинг\n",
    "\n",
    "####  Можно дописать еще всяких регулярок ну классической предобработки, но основной момент это увеличение  кол-ва токенов хотя бы до 256 - пока не позволяют мощности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "becd902e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc281640d9f1482b9fe5ad8b014a09a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MAX_LENGTH = 128\n",
    "def preprocess_function(examples):\n",
    "    result = tokenizer(\n",
    "        examples['full_text'],\n",
    "        padding='max_length', max_length=MAX_LENGTH, truncation=True)\n",
    "    \n",
    "    result['cohesion'] = examples['cohesion']\n",
    "    result['syntax'] = examples['syntax']\n",
    "    result['vocabulary'] = examples['vocabulary']\n",
    "    result['phraseology'] = examples['phraseology']\n",
    "    result['grammar'] = examples['grammar']\n",
    "    result['conventions'] = examples['conventions']\n",
    "    \n",
    "    return result\n",
    "\n",
    "train_hf_preprocessed = train_hf.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01cea7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6529ab3",
   "metadata": {},
   "source": [
    "## Обучение модели \n",
    "\n",
    "#### Хочу - unfreeze 50-100  слоев + автоэнкодер + другие конфиги на lr и параметры - пока тоже не позволяет комп"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a136477f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTuningUnfreeze(LightningModule):\n",
    "    def __init__(self, model_name = \"microsoft/deberta-v3-base\"):\n",
    "        super().__init__()\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "        self.bert = transformers.AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad=False  \n",
    "            \n",
    "        self.last = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1))\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "        #cnt_params = 0\n",
    "        #for _ in self.bert.parameters():\n",
    "            #cnt_params += 1\n",
    "        \n",
    "        #for i, param in enumerate(self.bert.parameters()):\n",
    "            #if i < cnt_params - unfreeze_last:\n",
    "                #param.requires_grad = False\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "    def forward(self, batch):\n",
    "        bert_output = self.bert(\n",
    "            input_ids=batch['input_ids'].to(device),\n",
    "            attention_mask=batch['attention_mask'].to(device),\n",
    "            token_type_ids=batch['token_type_ids'].to(device)\n",
    "        )['last_hidden_state']\n",
    "        bert_output = torch.max(bert_output, dim=1).values\n",
    "\n",
    "        return self.last(bert_output).squeeze(1)\n",
    "\n",
    "    def training_step(self, batch, *args):\n",
    "        pred = self.forward(batch)\n",
    "        loss = self.criterion(pred, batch['cohesion'].to(device) + 0.0) # tensor float\n",
    "        self.log(\"training_loss\", loss.item())\n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validation_step(self, batch, *args):\n",
    "        pred = self.forward(batch)\n",
    "        predicted = pred.cpu().detach().numpy()\n",
    "        target = batch['cohesion'].cpu().detach().numpy()\n",
    "        ans = mean_squared_error(target, predicted, squared=False)\n",
    "        self.log(\"val_rmse\", ans)\n",
    "        return ans\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=3e-5)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3598637d",
   "metadata": {},
   "source": [
    "def configure_optimizers(self):\n",
    "    optimizer = torch.optim.Adam([{\"params\": self.bert.parameters(), \"lr\": 3e-5}, \n",
    "                              {\"params\": self.last.parameters()}], lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3ed2c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = ModelTuningUnfreeze().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "519a0b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ну само собой разделю позже на тест трейн"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "42c371be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_hf_preprocessed['train'], batch_size= 32, collate_fn=transformers.default_data_collator, num_workers = 8)\n",
    "valid_loader = torch.utils.data.DataLoader(train_hf_preprocessed['train'], batch_size=256, collate_fn=transformers.default_data_collator, num_workers = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14ac3d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/belyakov/miniforge3/envs/env_tf/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:446: LightningDeprecationWarning: Setting `Trainer(gpus=0)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=0)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type           | Params\n",
      "---------------------------------------------\n",
      "0 | bert      | DebertaV2Model | 183 M \n",
      "1 | last      | Sequential     | 205 K \n",
      "2 | criterion | MSELoss        | 0     \n",
      "---------------------------------------------\n",
      "205 K     Trainable params\n",
      "183 M     Non-trainable params\n",
      "184 M     Total params\n",
      "736.147   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a9a8a9b58e34effb621851222f6d03f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(max_epochs=10, gpus=torch.cuda.device_count(), gradient_clip_val=0.1)\n",
    "trainer.fit(model, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81be6e9",
   "metadata": {},
   "source": [
    "## Модель - parto dos\n",
    "\n",
    "#### Хочу дописать голову со свертками либо сделать быстрый энкодер-декодер текстов, но это сущие понты для кагла по сравнению с глобальной проблемой - невозможностью использовать архитектуру берта на хотя бы половину из-за мощностей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4804a74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# вариант с проекта на работе который себя показал очень даже даже "
   ]
  },
  {
   "cell_type": "raw",
   "id": "d3fe7150",
   "metadata": {},
   "source": [
    "class BERT_Module(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "        super(BERT_Module, self).__init__()\n",
    "        self.bert = bert \n",
    "        self.conv = nn.Conv2d(in_channels=25, out_channels= 25, kernel_size= (3,1024),stride = (4,1), padding = (2,1)) #, stride = (5,2), padding = (3,3)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=1, padding = 1)#, padding = (1,1)\n",
    "        self.flat = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(.05)\n",
    "        self.relu =  nn.SELU()\n",
    "        self.fc1 = nn.Linear(2475,256)\n",
    "        self.fc2 = nn.Linear(256,3)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, sent_id, mask):\n",
    "        _, _, all_layers = self.bert(sent_id, attention_mask=mask, output_hidden_states=True, return_dict=False) #return_dict=False)\n",
    "        x = torch.cat(all_layers, 0)\n",
    "        x = torch.transpose(torch.cat(tuple([t.unsqueeze(0) for t in all_layers]), 0), 0, 1)\n",
    "        del all_layers\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        #x = self.pool(self.dropout(self.relu(self.conv(self.dropout(x)))))\n",
    "        #x = self.fc(self.dropout(self.flat(self.dropout(x))))\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
